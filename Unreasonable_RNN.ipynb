{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Pranav Srinivas Venkatesh- 1678255\n",
        "### Thejas Thirthalingaiah- 1678000\n",
        "### Bharath Kumar Nagaraju- 1604533"
      ],
      "metadata": {
        "id": "viphTBjkUBTi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mfkk0ecMvxHe"
      },
      "source": [
        "Shakespeare sonnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hg7zOA3rq37"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdD4MpQXrz0m"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess data\n",
        "def load_shakespeare_dataset():\n",
        "    # URL where the Shakespeare dataset is hosted\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
        "\n",
        "    # Download the dataset\n",
        "    path_to_file = tf.keras.utils.get_file('shakespeare.txt', url)\n",
        "\n",
        "    # Read the dataset into memory\n",
        "    text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "    # Return the loaded dataset\n",
        "    return text\n",
        "\n",
        "# Load the dataset\n",
        "text = load_shakespeare_dataset()\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "# Prepare dataset\n",
        "max_length = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - max_length, step):\n",
        "    sentences.append(text[i: i + max_length])\n",
        "    next_chars.append(text[i + max_length])\n",
        "X = np.zeros((len(sentences), max_length, len(chars)), dtype=np.bool_)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool_)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        X[i, t, char_to_int[char]] = 1\n",
        "    y[i, char_to_int[next_chars[i]]] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTBjz9aGrzxg",
        "outputId": "9d7d0215-93f4-4153-8738-4d3d828a3731"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "2905/2905 [==============================] - 339s 116ms/step - loss: 2.3889\n",
            "Epoch 2/10\n",
            "2905/2905 [==============================] - 337s 116ms/step - loss: 1.9783\n",
            "Epoch 3/10\n",
            "2905/2905 [==============================] - 338s 116ms/step - loss: 1.8381\n",
            "Epoch 4/10\n",
            "2905/2905 [==============================] - 337s 116ms/step - loss: 1.7538\n",
            "Epoch 5/10\n",
            "2905/2905 [==============================] - 339s 117ms/step - loss: 1.6932\n",
            "Epoch 6/10\n",
            "2905/2905 [==============================] - 336s 116ms/step - loss: 1.6462\n",
            "Epoch 7/10\n",
            "2905/2905 [==============================] - 336s 116ms/step - loss: 1.6082\n",
            "Epoch 8/10\n",
            "2905/2905 [==============================] - 338s 116ms/step - loss: 1.5775\n",
            "Epoch 9/10\n",
            "2905/2905 [==============================] - 335s 115ms/step - loss: 1.5515\n",
            "Epoch 10/10\n",
            "2905/2905 [==============================] - 337s 116ms/step - loss: 1.5292\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e7ee3285480>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(max_length, len(chars))))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, batch_size=128, epochs=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGBZw6NF6SGy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sample(preds, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Sample an index from a probability array.\n",
        "\n",
        "    preds: Numpy array of character predictions.\n",
        "    temperature: Controls the randomness of predictions.\n",
        "                 Higher temperature -> more random. Lower -> more deterministic.\n",
        "    \"\"\"\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds + 1e-7) / temperature  # Avoid division by zero; add small value\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEum4q5Grzu-",
        "outputId": "092359fd-43d9-45de-830f-c5e4a4de94a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " be, or not to be: that is the question:\n",
            "This will for the comparest Tyerengeruse.\n",
            "\n",
            "INGlER:\n",
            "Are, i' kill for my precaiseteding\n",
            "Nor bride your causun, I devenges to you,\n",
            "Upon the kings?\n",
            "\n",
            "Auflel:\n",
            "Agasion out my partiman, this Remarst,\n",
            "Thur, hither sand yet un elt his suppero.\n",
            "\n",
            "hestneinsto's willowh our treat him: and that's fault\n",
            "The brother, himself, now, a hoops that I consity\n",
            "With her hee of your honour. Lether, sheew be monale\n",
            "them, I\n"
          ]
        }
      ],
      "source": [
        "# Function to generate text\n",
        "def generate_text(length, seed):\n",
        "    generated = ''\n",
        "    seed = seed[-max_length:]  # Take only the last max_length characters\n",
        "    generated += seed\n",
        "    for i in range(length):\n",
        "        x_pred = np.zeros((1, max_length, len(chars)))\n",
        "        for t, char in enumerate(seed):\n",
        "            x_pred[0, t, char_to_int[char]] = 1\n",
        "\n",
        "        preds = model.predict(x_pred, verbose=0)[0]\n",
        "        next_index = sample(preds)\n",
        "        next_char = int_to_char[next_index]\n",
        "\n",
        "        generated += next_char\n",
        "        seed = seed[1:] + next_char  # Shift the seed and add the new char\n",
        "\n",
        "    return generated\n",
        "\n",
        "\n",
        "# Generate text\n",
        "print(generate_text(400, \"To be, or not to be: that is the question:\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8spx04QRvuCr"
      },
      "source": [
        "Baby Name Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-InT-AbrzqN",
        "outputId": "4fbbec09-e080-4d6f-9f3e-0a4c4bcd3718"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EO8jaJ5rznu"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, GRU\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Alm7i_ZErzlZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Path to the folder containing your text files\n",
        "folder_path = '/content/drive/My Drive/Names'\n",
        "\n",
        "# List all files in the folder and filter for text files\n",
        "text_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.txt')]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5DQDVrOyNZX"
      },
      "outputs": [],
      "source": [
        "file_contents = []  # List to store the contents of each file\n",
        "\n",
        "for file_name in text_files:\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "        file_contents.append(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9FIV6Gfy9H7"
      },
      "outputs": [],
      "source": [
        "all_text = ' '.join(file_contents).lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWtD23dpzHyA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LM_d2S-zy9Fk"
      },
      "outputs": [],
      "source": [
        "# Tokenize characters\n",
        "tokenizer = Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts([all_text])\n",
        "\n",
        "# Convert text to sequences of integers\n",
        "sequences = tokenizer.texts_to_sequences([all_text])[0]\n",
        "\n",
        "# Vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Create sequences and labels\n",
        "seq_length = 10  # Length of input sequences\n",
        "sequences = [sequences[i:i + seq_length + 1] for i in range(len(sequences) - seq_length)]\n",
        "sequences = np.array(pad_sequences(sequences, maxlen=seq_length + 1, padding='pre'))\n",
        "\n",
        "# Prepare input and output\n",
        "X, y = sequences[:, :-1], sequences[:, -1]\n",
        "y = to_categorical(y, num_classes=vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wC1E0ajdIQHT",
        "outputId": "c1d55792-79fa-4e39-e1c2-28a606c79feb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 2, 16,  2, ...,  1,  2, 16],\n",
              "       [16,  2, 18, ...,  2, 16,  2],\n",
              "       [ 2, 18,  2, ..., 16,  2, 18],\n",
              "       ...,\n",
              "       [ 8,  6, 21, ..., 10, 16,  8],\n",
              "       [ 6, 21,  2, ..., 16,  8,  6],\n",
              "       [21,  2,  7, ...,  8,  6,  5]], dtype=int32)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6qqAmTjy9DO",
        "outputId": "27e0e20d-2227-40c8-d612-760b339f01b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "229/229 [==============================] - 32s 111ms/step - loss: 2.8679 - accuracy: 0.1462\n",
            "Epoch 2/100\n",
            "229/229 [==============================] - 25s 108ms/step - loss: 2.4983 - accuracy: 0.2573\n",
            "Epoch 3/100\n",
            "229/229 [==============================] - 27s 116ms/step - loss: 2.0746 - accuracy: 0.3839\n",
            "Epoch 4/100\n",
            "229/229 [==============================] - 23s 102ms/step - loss: 1.8391 - accuracy: 0.4653\n",
            "Epoch 5/100\n",
            "229/229 [==============================] - 25s 108ms/step - loss: 1.7086 - accuracy: 0.5069\n",
            "Epoch 6/100\n",
            "229/229 [==============================] - 25s 109ms/step - loss: 1.6155 - accuracy: 0.5361\n",
            "Epoch 7/100\n",
            "229/229 [==============================] - 25s 109ms/step - loss: 1.5392 - accuracy: 0.5596\n",
            "Epoch 8/100\n",
            "229/229 [==============================] - 23s 101ms/step - loss: 1.4766 - accuracy: 0.5766\n",
            "Epoch 9/100\n",
            "229/229 [==============================] - 25s 109ms/step - loss: 1.4189 - accuracy: 0.5922\n",
            "Epoch 10/100\n",
            "229/229 [==============================] - 25s 108ms/step - loss: 1.3675 - accuracy: 0.6038\n",
            "Epoch 11/100\n",
            "229/229 [==============================] - 24s 104ms/step - loss: 1.3189 - accuracy: 0.6160\n",
            "Epoch 12/100\n",
            "229/229 [==============================] - 24s 105ms/step - loss: 1.2767 - accuracy: 0.6280\n",
            "Epoch 13/100\n",
            "229/229 [==============================] - 25s 108ms/step - loss: 1.2342 - accuracy: 0.6367\n",
            "Epoch 14/100\n",
            "229/229 [==============================] - 25s 110ms/step - loss: 1.1956 - accuracy: 0.6477\n",
            "Epoch 15/100\n",
            "229/229 [==============================] - 23s 103ms/step - loss: 1.1587 - accuracy: 0.6583\n",
            "Epoch 16/100\n",
            "229/229 [==============================] - 25s 108ms/step - loss: 1.1241 - accuracy: 0.6642\n",
            "Epoch 17/100\n",
            "229/229 [==============================] - 25s 108ms/step - loss: 1.0878 - accuracy: 0.6741\n",
            "Epoch 18/100\n",
            "229/229 [==============================] - 25s 109ms/step - loss: 1.0534 - accuracy: 0.6834\n",
            "Epoch 19/100\n",
            "229/229 [==============================] - 23s 101ms/step - loss: 1.0213 - accuracy: 0.6915\n",
            "Epoch 20/100\n",
            "229/229 [==============================] - 25s 108ms/step - loss: 0.9899 - accuracy: 0.6989\n",
            "Epoch 21/100\n",
            "229/229 [==============================] - 25s 110ms/step - loss: 0.9561 - accuracy: 0.7099\n",
            "Epoch 22/100\n",
            "229/229 [==============================] - 24s 106ms/step - loss: 0.9253 - accuracy: 0.7169\n",
            "Epoch 23/100\n",
            "229/229 [==============================] - 24s 103ms/step - loss: 0.8938 - accuracy: 0.7260\n",
            "Epoch 24/100\n",
            "229/229 [==============================] - 25s 109ms/step - loss: 0.8625 - accuracy: 0.7348\n",
            "Epoch 25/100\n",
            "229/229 [==============================] - 25s 109ms/step - loss: 0.8311 - accuracy: 0.7437\n",
            "Epoch 26/100\n",
            "229/229 [==============================] - 23s 102ms/step - loss: 0.8013 - accuracy: 0.7508\n",
            "Epoch 27/100\n",
            "229/229 [==============================] - 24s 106ms/step - loss: 0.7714 - accuracy: 0.7613\n",
            "Epoch 28/100\n",
            "229/229 [==============================] - 25s 108ms/step - loss: 0.7440 - accuracy: 0.7688\n",
            "Epoch 29/100\n",
            "229/229 [==============================] - 24s 106ms/step - loss: 0.7133 - accuracy: 0.7793\n",
            "Epoch 30/100\n",
            "229/229 [==============================] - 23s 101ms/step - loss: 0.6836 - accuracy: 0.7867\n",
            "Epoch 31/100\n",
            "229/229 [==============================] - 25s 111ms/step - loss: 0.6546 - accuracy: 0.7960\n",
            "Epoch 32/100\n",
            "229/229 [==============================] - 25s 111ms/step - loss: 0.6296 - accuracy: 0.8026\n",
            "Epoch 33/100\n",
            "229/229 [==============================] - 25s 111ms/step - loss: 0.6010 - accuracy: 0.8110\n",
            "Epoch 34/100\n",
            "229/229 [==============================] - 23s 101ms/step - loss: 0.5748 - accuracy: 0.8206\n",
            "Epoch 35/100\n",
            "229/229 [==============================] - 25s 109ms/step - loss: 0.5445 - accuracy: 0.8298\n",
            "Epoch 36/100\n",
            "229/229 [==============================] - 25s 109ms/step - loss: 0.5222 - accuracy: 0.8367\n",
            "Epoch 37/100\n",
            "229/229 [==============================] - 25s 107ms/step - loss: 0.4976 - accuracy: 0.8436\n",
            "Epoch 38/100\n",
            "229/229 [==============================] - 24s 105ms/step - loss: 0.4723 - accuracy: 0.8519\n",
            "Epoch 39/100\n",
            "229/229 [==============================] - 25s 109ms/step - loss: 0.4485 - accuracy: 0.8600\n",
            "Epoch 40/100\n",
            "229/229 [==============================] - 25s 109ms/step - loss: 0.4242 - accuracy: 0.8680\n",
            "Epoch 41/100\n",
            "229/229 [==============================] - 24s 107ms/step - loss: 0.4022 - accuracy: 0.8754\n",
            "Epoch 42/100\n",
            "229/229 [==============================] - 24s 106ms/step - loss: 0.3801 - accuracy: 0.8827\n",
            "Epoch 43/100\n",
            "229/229 [==============================] - 26s 112ms/step - loss: 0.3604 - accuracy: 0.8874\n",
            "Epoch 44/100\n",
            "229/229 [==============================] - 26s 112ms/step - loss: 0.3374 - accuracy: 0.8962\n",
            "Epoch 45/100\n",
            "229/229 [==============================] - 25s 110ms/step - loss: 0.3202 - accuracy: 0.9024\n",
            "Epoch 46/100\n",
            "229/229 [==============================] - 23s 102ms/step - loss: 0.2988 - accuracy: 0.9096\n",
            "Epoch 47/100\n",
            "229/229 [==============================] - 25s 109ms/step - loss: 0.2826 - accuracy: 0.9144\n",
            "Epoch 48/100\n",
            "229/229 [==============================] - 25s 110ms/step - loss: 0.2714 - accuracy: 0.9186\n",
            "Epoch 49/100\n",
            "229/229 [==============================] - 25s 108ms/step - loss: 0.2529 - accuracy: 0.9243\n",
            "Epoch 50/100\n",
            "229/229 [==============================] - 24s 103ms/step - loss: 0.2406 - accuracy: 0.9281\n",
            "Epoch 51/100\n",
            "229/229 [==============================] - 25s 108ms/step - loss: 0.2245 - accuracy: 0.9333\n",
            "Epoch 52/100\n",
            "229/229 [==============================] - 25s 108ms/step - loss: 0.2140 - accuracy: 0.9365\n",
            "Epoch 53/100\n",
            "229/229 [==============================] - 24s 106ms/step - loss: 0.1973 - accuracy: 0.9420\n",
            "Epoch 54/100\n",
            "229/229 [==============================] - 23s 101ms/step - loss: 0.1886 - accuracy: 0.9448\n",
            "Epoch 55/100\n",
            "229/229 [==============================] - 25s 109ms/step - loss: 0.1846 - accuracy: 0.9463\n",
            "Epoch 56/100\n",
            "229/229 [==============================] - 25s 109ms/step - loss: 0.1705 - accuracy: 0.9513\n",
            "Epoch 57/100\n",
            "229/229 [==============================] - 28s 122ms/step - loss: 0.1593 - accuracy: 0.9553\n",
            "Epoch 58/100\n",
            "229/229 [==============================] - 23s 101ms/step - loss: 0.1527 - accuracy: 0.9572\n",
            "Epoch 59/100\n",
            "229/229 [==============================] - 25s 108ms/step - loss: 0.1576 - accuracy: 0.9537\n",
            "Epoch 60/100\n",
            "229/229 [==============================] - 25s 108ms/step - loss: 0.1497 - accuracy: 0.9572\n",
            "Epoch 61/100\n",
            "229/229 [==============================] - 25s 110ms/step - loss: 0.1358 - accuracy: 0.9615\n",
            "Epoch 62/100\n",
            "229/229 [==============================] - 24s 104ms/step - loss: 0.1288 - accuracy: 0.9648\n",
            "Epoch 63/100\n",
            "229/229 [==============================] - 24s 107ms/step - loss: 0.1292 - accuracy: 0.9627\n",
            "Epoch 64/100\n",
            "229/229 [==============================] - 25s 108ms/step - loss: 0.1285 - accuracy: 0.9631\n",
            "Epoch 65/100\n",
            "229/229 [==============================] - 23s 102ms/step - loss: 0.1214 - accuracy: 0.9656\n",
            "Epoch 66/100\n",
            "229/229 [==============================] - 24s 106ms/step - loss: 0.1173 - accuracy: 0.9680\n",
            "Epoch 67/100\n",
            "229/229 [==============================] - 25s 109ms/step - loss: 0.1110 - accuracy: 0.9696\n",
            "Epoch 68/100\n",
            "229/229 [==============================] - 25s 108ms/step - loss: 0.1077 - accuracy: 0.9700\n",
            "Epoch 69/100\n",
            "229/229 [==============================] - 23s 102ms/step - loss: 0.1186 - accuracy: 0.9661\n",
            "Epoch 70/100\n",
            "229/229 [==============================] - 25s 110ms/step - loss: 0.1087 - accuracy: 0.9678\n",
            "Epoch 71/100\n",
            "229/229 [==============================] - 25s 109ms/step - loss: 0.1026 - accuracy: 0.9718\n",
            "Epoch 72/100\n",
            "229/229 [==============================] - 25s 109ms/step - loss: 0.0989 - accuracy: 0.9726\n",
            "Epoch 73/100\n",
            "229/229 [==============================] - 23s 101ms/step - loss: 0.1000 - accuracy: 0.9714\n",
            "Epoch 74/100\n",
            "229/229 [==============================] - 26s 112ms/step - loss: 0.1033 - accuracy: 0.9704\n",
            "Epoch 75/100\n",
            "229/229 [==============================] - 25s 110ms/step - loss: 0.0962 - accuracy: 0.9726\n",
            "Epoch 76/100\n",
            "229/229 [==============================] - 25s 110ms/step - loss: 0.0951 - accuracy: 0.9738\n",
            "Epoch 77/100\n",
            "229/229 [==============================] - 23s 103ms/step - loss: 0.0925 - accuracy: 0.9740\n",
            "Epoch 78/100\n",
            "229/229 [==============================] - 25s 109ms/step - loss: 0.0994 - accuracy: 0.9705\n",
            "Epoch 79/100\n",
            "229/229 [==============================] - 25s 108ms/step - loss: 0.0936 - accuracy: 0.9738\n",
            "Epoch 80/100\n",
            "229/229 [==============================] - 25s 108ms/step - loss: 0.0885 - accuracy: 0.9747\n",
            "Epoch 81/100\n",
            "229/229 [==============================] - 24s 104ms/step - loss: 0.0873 - accuracy: 0.9752\n",
            "Epoch 82/100\n",
            "229/229 [==============================] - 26s 112ms/step - loss: 0.0836 - accuracy: 0.9768\n",
            "Epoch 83/100\n",
            "229/229 [==============================] - 25s 110ms/step - loss: 0.0881 - accuracy: 0.9751\n",
            "Epoch 84/100\n",
            "229/229 [==============================] - 25s 110ms/step - loss: 0.0946 - accuracy: 0.9717\n",
            "Epoch 85/100\n",
            "229/229 [==============================] - 23s 102ms/step - loss: 0.0945 - accuracy: 0.9717\n",
            "Epoch 86/100\n",
            "229/229 [==============================] - 25s 111ms/step - loss: 0.0871 - accuracy: 0.9743\n",
            "Epoch 87/100\n",
            "229/229 [==============================] - 25s 110ms/step - loss: 0.0824 - accuracy: 0.9764\n",
            "Epoch 88/100\n",
            "229/229 [==============================] - 25s 109ms/step - loss: 0.0813 - accuracy: 0.9773\n",
            "Epoch 89/100\n",
            "229/229 [==============================] - 23s 102ms/step - loss: 0.0731 - accuracy: 0.9802\n",
            "Epoch 90/100\n",
            "229/229 [==============================] - 25s 109ms/step - loss: 0.0753 - accuracy: 0.9801\n",
            "Epoch 91/100\n",
            "229/229 [==============================] - 25s 109ms/step - loss: 0.0787 - accuracy: 0.9780\n",
            "Epoch 92/100\n",
            "229/229 [==============================] - 25s 111ms/step - loss: 0.0868 - accuracy: 0.9736\n",
            "Epoch 93/100\n",
            "229/229 [==============================] - 24s 103ms/step - loss: 0.0880 - accuracy: 0.9744\n",
            "Epoch 94/100\n",
            "229/229 [==============================] - 25s 109ms/step - loss: 0.0805 - accuracy: 0.9759\n",
            "Epoch 94: early stopping\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7db874ebb580>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "\n",
        "# Build the model\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, 50, input_length=seq_length),\n",
        "    LSTM(100, return_sequences=True),\n",
        "    LSTM(100),\n",
        "    Dense(100, activation='relu'),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# EarlyStopping callback\n",
        "early_stopping = EarlyStopping(monitor='loss', patience=5, verbose=1)\n",
        "\n",
        "# Train the model with EarlyStopping\n",
        "model.fit(X, y, batch_size=256, epochs=100, callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UzHj3yky8-j",
        "outputId": "2d7bf320-1790-4be1-f1fb-bc2d7d7a15d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Talesse\n",
            "gatoli\n"
          ]
        }
      ],
      "source": [
        "def generate_name(model, tokenizer, seed_text=\"A\", max_len=20):\n",
        "    generated_name = seed_text\n",
        "    for _ in range(max_len):\n",
        "        # Preprocess the seed_text\n",
        "        encoded = tokenizer.texts_to_sequences([generated_name])[-1]\n",
        "        encoded = pad_sequences([encoded], maxlen=seq_length, padding='pre', truncating='pre')\n",
        "\n",
        "        # Predict the next character\n",
        "        prediction = model.predict(encoded, verbose=0).flatten()\n",
        "        next_index = np.argmax(prediction)\n",
        "        next_char = tokenizer.index_word.get(next_index, '')\n",
        "\n",
        "        # Break if the model fails to predict a new character\n",
        "        if next_char == '':\n",
        "            break\n",
        "\n",
        "        generated_name += next_char\n",
        "\n",
        "    return generated_name\n",
        "\n",
        "# Example usage\n",
        "print(generate_name(model, tokenizer, seed_text=\"Tale\", max_len=10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mrByK22_nXQ"
      },
      "source": [
        "Latex Code Generator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmEtuCuvOeAd"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwEJUiH0_qLZ",
        "outputId": "f50661f8-a6ff-4269-cbf0-3c75b552a01b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnJ5uXa__vgj"
      },
      "outputs": [],
      "source": [
        "# Replace 'path_to_your_file.txt' with the actual path to your LaTeX file in Google Drive\n",
        "folder_path = '/content/drive/My Drive/Names/Latex'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFffQSmqAFI8"
      },
      "outputs": [],
      "source": [
        "# List all files in the folder and filter for text files\n",
        "text_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.txt')]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsNClIPcBfkQ",
        "outputId": "fbfd469d-8235-4941-a747-11bb4b3ffaa3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/Names/Latex/file.txt']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6ErufUVBljl"
      },
      "outputs": [],
      "source": [
        "data = []  # List to store the contents of each file\n",
        "\n",
        "for file_name in text_files:\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "        data.append(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgfLeR52BqTu"
      },
      "outputs": [],
      "source": [
        "#data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PPlDHWxBF8G"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzJ9rARVCfFx",
        "outputId": "b3205234-a613-432b-f190-8ed383846c34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset length: 1\n",
            "First 100 characters of dataset: ['\\\\documentclass{article}\\n\\\\usepackage{graphicx}\\n\\\\usepackage{amsmath}\\n\\\\usepackage{hyperref}\\n\\n\\\\title{Sample LaTeX Document}\\n\\\\author{Your Name}\\n\\\\date{\\\\today}\\n\\n\\\\begin{document}\\n\\n\\\\maketitle\\n\\n\\\\section{Introduction}\\nThis is a sample LaTeX document demonstrating various features.\\n\\n\\\\section{Equations}\\nHere\\'s a simple equation:\\n\\\\begin{equation}\\n    E = mc^2\\n\\\\end{equation}\\nAnd here\\'s another one:\\n\\\\begin{equation}\\n    F = ma\\n\\\\end{equation}\\n\\n\\\\section{Lists}\\n\\\\subsection{Bullet Points}\\n\\\\begin{itemize}\\n    \\\\item Item 1\\n    \\\\item Item 2\\n    \\\\item Item 3\\n\\\\end{itemize}\\n\\n\\\\subsection{Numbered List}\\n\\\\begin{enumerate}\\n    \\\\item First item\\n    \\\\item Second item\\n    \\\\item Third item\\n\\\\end{enumerate}\\n\\n\\\\section{Figures}\\n\\\\begin{figure}[h]\\n    \\\\centering\\n    \\\\includegraphics[width=0.5\\\\textwidth]{example-image-a}\\n    \\\\caption{Example Figure}\\n    \\\\label{fig:example}\\n\\\\end{figure}\\n\\nFigure \\\\ref{fig:example} shows an example figure.\\n\\n\\\\section{References}\\nWe referenced Figure \\\\ref{fig:example} in the previous section.\\n\\nFor more information on LaTeX, check out \\\\cite{latexcompanion}.\\n\\n\\\\bibliographystyle{plain}\\n\\\\bibliography{references}\\n\\n\\\\end{document}\\n\\\\documentclass{article}\\n\\\\usepackage{graphicx}\\n\\\\usepackage{amsmath}\\n\\\\usepackage{amsfonts}\\n\\\\usepackage{amssymb}\\n\\\\usepackage{listings}\\n\\\\usepackage{color}\\n\\\\usepackage{hyperref}\\n\\\\usepackage{lipsum}\\n\\n\\\\usepackage{graphicx}\\n\\\\usepackage{amsmath}\\n\\\\usepackage{amsfonts}\\n\\\\usepackage{amssymb}\\n\\\\usepackage{listings}\\n\\\\usepackage{xcolor}\\n\\\\usepackage{hyperref}\\n\\\\usepackage{lipsum}\\n\\\\usepackage{enumitem}\\n\\\\usepackage{float}\\n\\\\usepackage{tikz}\\n\\\\usepackage{pgfplots}\\n\\\\usepackage{caption}\\n\\\\usepackage{subcaption}\\n\\\\usepackage{geometry}\\n\\\\usepackage{booktabs}\\n\\\\usepackage{multirow}\\n\\n\\\\title{Sample LaTeX Document}\\n\\\\author{Your Name}\\n\\\\date{\\\\today}\\n\\n\\\\begin{document}\\n\\n\\\\maketitle\\n\\n\\\\section{Introduction}\\nThis is a sample LaTeX document showcasing various features.\\n\\n\\\\section{Equations}\\n\\\\subsection{Inline Equation}\\nHere is an example of an inline equation: $E=mc^2$.\\n\\n\\\\subsection{Display Equation}\\nHere is an example of a display equation:\\n\\\\begin{equation}\\n    \\\\int_{0}^{\\\\infty} e^{-x^2} dx = \\\\frac{\\\\sqrt{\\\\pi}}{2}\\n\\\\end{equation}\\n\\n\\\\section{Lists}\\n\\\\subsection{Unordered List}\\n\\\\begin{itemize}\\n    \\\\item Item 1\\n    \\\\item Item 2\\n    \\\\item Item 3\\n\\\\end{itemize}\\n\\n\\\\subsection{Ordered List}\\n\\\\begin{enumerate}\\n    \\\\item First item\\n    \\\\item Second item\\n    \\\\item Third item\\n\\\\end{enumerate}\\n\\n\\\\section{Tables}\\n\\\\begin{table}[htbp]\\n    \\\\centering\\n    \\\\begin{tabular}{|c|c|}\\n        \\\\hline\\n        \\\\textbf{Name} & \\\\textbf{Age} \\\\\\\\\\n        \\\\hline\\n        John & 25 \\\\\\\\\\n        Alice & 30 \\\\\\\\\\n        Bob & 28 \\\\\\\\\\n        \\\\hline\\n    \\\\end{tabular}\\n    \\\\caption{Sample Table}\\n    \\\\label{tab:sample-table}\\n\\\\end{table}\\n\\nTable \\\\ref{tab:sample-table} shows a sample table.\\n\\n\\\\section{Figures}\\n\\\\begin{figure}[htbp]\\n    \\\\centering\\n    \\\\includegraphics[width=0.5\\\\textwidth]{example-image}\\n    \\\\caption{Sample Figure}\\n    \\\\label{fig:sample-figure}\\n\\\\end{figure}\\n\\nFigure \\\\ref{fig:sample-figure} shows a sample figure.\\n\\n\\\\section{Code}\\n\\\\subsection{Inline Code}\\nHere is an example of inline code: \\\\texttt{print(\"Hello, world!\")}.\\n\\n\\\\subsection{Code Listing}\\n\\\\begin{lstlisting}[language=Python, caption={Sample Python Code}, label={lst:sample-code}]\\ndef fibonacci(n):\\n    if n <= 1:\\n        return n\\n    else:\\n        return fibonacci(n-1) + fibonacci(n-2)\\n\\nprint(fibonacci(5))\\n\\\\end{lstlisting}\\n\\nListing \\\\ref{lst:sample-code} shows a sample Python code.\\n\\n\\\\section{References}\\nThis document can be referenced using \\\\texttt{ref} and \\\\texttt{label} commands.\\n\\n\\\\section{Hyperlinks}\\n\\\\href{https://www.example.com}{Click here} to visit Example.com.\\n\\n\\\\section{Lorem Ipsum}\\n\\\\lipsum[1-3]\\n\\n\\\\section{Introduction}\\nThis is a big sample LaTeX document showcasing various features.\\n\\n\\\\section{Equations}\\n\\\\subsection{Inline Equation}\\nHere is an example of an inline equation: $E=mc^2$.\\n\\n\\\\subsection{Display Equations}\\nHere are some display equations:\\n\\\\begin{align}\\n    \\\\int_{0}^{\\\\infty} e^{-x^2} dx &= \\\\frac{\\\\sqrt{\\\\pi}}{2} \\\\\\\\\\n    F &= ma \\\\\\\\\\n    \\\\nabla \\\\cdot \\\\mathbf{E} &= \\\\frac{\\\\rho}{\\\\varepsilon_0}\\n\\\\end{align}\\n\\n\\\\section{Lists}\\n\\\\subsection{Unordered List}\\n\\\\begin{itemize}[label=$\\\\diamond$]\\n    \\\\item Item 1\\n    \\\\item Item 2\\n    \\\\item Item 3\\n\\\\end{itemize}\\n\\n\\\\subsection{Ordered List}\\n\\\\begin{enumerate}[label=\\\\roman*.]\\n    \\\\item First item\\n    \\\\item Second item\\n    \\\\item Third item\\n\\\\end{enumerate}\\n\\n\\\\section{Tables}\\n\\\\begin{table}[H]\\n    \\\\centering\\n    \\\\begin{tabular}{cc}\\n        \\\\toprule\\n        \\\\textbf{Name} & \\\\textbf{Age} \\\\\\\\\\n        \\\\midrule\\n        John & 25 \\\\\\\\\\n        Alice & 30 \\\\\\\\\\n        Bob & 28 \\\\\\\\\\n        \\\\bottomrule\\n    \\\\end{tabular}\\n    \\\\caption{Sample Table}\\n    \\\\label{tab:sample-table}\\n\\\\end{table}\\n\\nTable \\\\ref{tab:sample-table} shows a sample table.\\n\\n\\\\section{Figures}\\n\\\\begin{figure}[H]\\n    \\\\centering\\n    \\\\begin{subfigure}[b]{0.45\\\\textwidth}\\n        \\\\centering\\n        \\\\includegraphics[width=\\\\textwidth]{example-image-a}\\n        \\\\caption{Subfigure A}\\n        \\\\label{fig:subfig-a}\\n    \\\\end{subfigure}\\n    \\\\hfill\\n    \\\\begin{subfigure}[b]{0.45\\\\textwidth}\\n        \\\\centering\\n        \\\\includegraphics[width=\\\\textwidth]{example-image-b}\\n        \\\\caption{Subfigure B}\\n        \\\\label{fig:subfig-b}\\n    \\\\end{subfigure}\\n    \\\\caption{Sample Figure with Subfigures}\\n    \\\\label{fig:sample-fig}\\n\\\\end{figure}\\n\\nFigure \\\\ref{fig:sample-fig} shows a sample figure with subfigures \\\\ref{fig:subfig-a} and \\\\ref{fig:subfig-b}.\\n\\n\\\\section{Code}\\n\\\\subsection{Inline Code}\\nHere is an example of inline code: \\\\texttt{print(\"Hello, world!\")}.\\n\\n\\\\subsection{Code Listing}\\n\\\\begin{lstlisting}[language=Python, caption={Sample Python Code}, label={lst:sample-code}]\\ndef example_function(x):\\n    return x**2\\n\\nresult = example_function(5)\\nprint(\"Result:\", result)\\n\\\\end{lstlisting}\\n\\nListing \\\\ref{lst:sample-code} shows a sample Python code.\\n\\n\\\\section{References}\\nThis document can be referenced using \\\\texttt{ref} and \\\\texttt{label} commands.\\n\\n\\\\section{Hyperlinks}\\n\\\\href{https://www.example.com}{Click here} to visit Example.com.\\n\\n\\\\section{Lorem Ipsum}\\n\\\\lipsum[1-4]\\n\\n\\\\end{document}\\n\\\\documentclass{article}\\n\\\\usepackage{graphicx}\\n\\\\usepackage{amsmath}\\n\\\\usepackage{hyperref}\\n\\\\usepackage{lipsum} % for generating dummy text\\n\\\\usepackage{multirow} % for multirow cells in tables\\n\\\\usepackage{float} % for better figure placement using [H]\\n\\n\\\\title{Sample LaTeX Document}\\n\\\\author{Your Name}\\n\\\\date{\\\\today}\\n\\n\\\\begin{document}\\n\\n\\\\maketitle\\n\\n\\\\begin{abstract}\\nThis is an abstract. \\\\lipsum[1]\\n\\\\end{abstract}\\n\\n\\\\section{Introduction}\\n\\\\lipsum[2-3]\\n\\n\\\\section{Literature Review}\\n\\\\subsection{Subsection 1}\\n\\\\lipsum[4]\\n\\\\subsection{Subsection 2}\\n\\\\lipsum[5]\\n\\n\\\\section{Methodology}\\n\\\\subsection{Data Collection}\\n\\\\lipsum[6]\\n\\\\subsection{Data Analysis}\\n\\\\lipsum[7]\\n\\n\\\\section{Results}\\n\\\\lipsum[8]\\n\\n\\\\begin{table}[H]\\n\\\\centering\\n\\\\caption{Sample Table}\\n\\\\begin{tabular}{|c|c|c|}\\n\\\\hline\\n\\\\textbf{Column 1} & \\\\textbf{Column 2} & \\\\textbf{Column 3} \\\\\\\\ \\\\hline\\n1 & A & X \\\\\\\\ \\\\hline\\n2 & B & Y \\\\\\\\ \\\\hline\\n3 & C & Z \\\\\\\\ \\\\hline\\n\\\\end{tabular}\\n\\\\label{tab:sample}\\n\\\\end{table}\\n\\nTable \\\\ref{tab:sample} shows a sample table.\\n\\n\\\\section{Discussion}\\n\\\\lipsum[9]\\n\\n\\\\section{Conclusion}\\n\\\\lipsum[10]\\n\\n\\\\section*{Acknowledgments}\\n\\\\lipsum[11]\\n\\n\\\\section*{References}\\n\\\\begin{enumerate}\\n    \\\\item Author A, et al. (Year). Title of the paper. \\\\textit{Journal Name}, Volume(Issue), Page numbers. \\\\url{https://doi.org/xxxxx}\\n    \\\\item Author B, et al. (Year). Title of the paper. \\\\textit{Journal Name}, Volume(Issue), Page numbers. \\\\url{https://doi.org/yyyyy}\\n\\\\end{enumerate}\\n\\n\\\\end{document}\\n\\\\documentclass{article}\\n\\\\usepackage{graphicx}\\n\\\\usepackage{amsmath}\\n\\\\usepackage{algorithm}\\n\\\\usepackage{algpseudocode}\\n\\\\usepackage{listings}\\n\\n\\\\title{Advanced LaTeX Example}\\n\\\\author{Your Name}\\n\\\\date{\\\\today}\\n\\n\\\\begin{document}\\n\\n\\\\maketitle\\n\\n\\\\section{Introduction}\\nLaTeX is a powerful typesetting system for producing scientific documents.\\n\\n\\\\section{Mathematics}\\nHere\\'s an example of an inline equation: $e^{i\\\\pi} + 1 = 0$. And here\\'s a displayed equation:\\n\\\\begin{equation}\\n    \\\\int_{0}^{\\\\pi} \\\\sin(x) \\\\, dx = 2\\n\\\\end{equation}\\n\\n\\\\section{Algorithms}\\n\\\\subsection{Pseudocode}\\n\\\\begin{algorithm}\\n\\\\caption{Euclid\\'s algorithm}\\\\label{euclid}\\n\\\\begin{algorithmic}[1]\\n\\\\Procedure{Euclid}{$a,b$}\\\\Comment{The gcd of $a$ and $b$}\\n\\\\State $r\\\\gets a\\\\bmod b$\\n\\\\While{$r\\\\not=0$}\\\\Comment{We have the answer if $r$ is 0}\\n\\\\State $a\\\\gets b$\\n\\\\State $b\\\\gets r$\\n\\\\State $r\\\\gets a\\\\bmod b$\\n\\\\EndWhile\\\\label{euclidendwhile}\\n\\\\State \\\\textbf{return} $b$\\\\Comment{The gcd is $b$}\\n\\\\EndProcedure\\n\\\\end{algorithmic}\\n\\\\end{algorithm}\\n\\n\\\\section{Code Listings}\\n\\\\subsection{Python}\\n\\\\lstset{language=Python}\\n\\\\begin{lstlisting}\\ndef fibonacci(n):\\n    if n <= 1:\\n        return n\\n    else:\\n        return fibonacci(n-1) + fibonacci(n-2)\\n\\nprint(fibonacci(10))\\n\\\\end{lstlisting}\\n\\n\\\\subsection{Java}\\n\\\\lstset{language=Java}\\n\\\\begin{lstlisting}\\npublic class HelloWorld {\\n    public static void main(String[] args) {\\n        System.out.println(\"Hello, world!\");\\n    }\\n}\\n\\\\end{lstlisting}\\n\\n\\\\section{Conclusion}\\nLaTeX provides excellent support for mathematical typesetting, algorithmic pseudocode, and code listings.\\n\\n\\\\end{document}\\n\\\\documentclass[final,hyperref={pdfpagelabels=false}]{beamer}\\n\\\\mode<presentation>{\\\\usetheme{Berlin}}\\n\\\\usepackage[orientation=landscape,size=custom,width=120,height=90,scale=1.5,debug]{beamerposter} \\n\\\\usepackage{graphicx} \\n\\\\usepackage{booktabs} \\n\\\\usepackage{tikz}\\n\\\\usetikzlibrary{calc}\\n\\n\\\\title{Sample Beamer Poster}\\n\\\\author{Your Name}\\n\\\\institute{Your Institution}\\n\\n\\\\begin{document}\\n\\\\begin{frame}\\n\\\\begin{block}{Introduction}\\nThis is a sample Beamer poster illustrating various features such as columns, blocks, and figures.\\n\\\\end{block}\\n\\n\\\\begin{columns}[t]\\n\\n\\\\begin{column}{.48\\\\linewidth}\\n\\\\begin{block}{Methodology}\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed dignissim turpis a nisi placerat faucibus. Vestibulum quis sollicitudin dolor. \\n\\\\end{block}\\n\\n\\\\begin{block}{Results}\\n\\\\begin{figure}\\n\\\\centering\\n\\\\includegraphics[width=0.7\\\\linewidth]{example-image-a}\\n\\\\caption{Sample Figure}\\n\\\\label{fig:sample}\\n\\\\end{figure}\\n\\\\end{block}\\n\\\\end{column}\\n\\n\\\\begin{column}{.48\\\\linewidth}\\n\\\\begin{block}{Discussion}\\nSed at arcu in ligula vehicula fermentum. Morbi posuere diam a est consequat, eget condimentum risus malesuada. \\n\\\\end{block}\\n\\n\\\\begin{block}{Conclusion}\\nPellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Ut nec velit lobortis, sollicitudin nisl at, molestie ex. \\n\\\\end{block}\\n\\\\end{column}\\n\\\\end{columns}\\n\\n\\\\begin{block}{References}\\n\\\\footnotesize\\n\\\\begin{thebibliography}{99}\\n\\\\bibitem{author1990} A. Author. \\\\emph{Title of the paper}. Journal name, Year.\\n\\\\bibitem{author2000} B. Author. \\\\emph{Title of the paper}. Journal name, Year.\\n\\\\end{thebibliography}\\n\\\\end{block}\\n\\\\end{frame}\\n\\\\end{document}\\n\\\\documentclass{article}\\n\\\\usepackage{amsmath}\\n\\\\usepackage{amssymb}\\n\\\\usepackage{tikz}\\n\\\\usetikzlibrary{shapes.geometric, arrows}\\n\\n% Define block styles\\n\\\\tikzstyle{decision} = [diamond, draw, fill=blue!20, \\n    text width=4.5em, text badly centered, node distance=3cm, inner sep=0pt]\\n\\\\tikzstyle{block} = [rectangle, draw, fill=blue!20, \\n    text width=5em, text centered, rounded corners, minimum height=4em]\\n\\\\tikzstyle{line} = [draw, -latex\\']\\n\\\\tikzstyle{cloud} = [draw, ellipse,fill=red!20, node distance=3cm,\\n    minimum height=2em]\\n\\n\\\\title{Complex LaTeX Example}\\n\\\\author{Your Name}\\n\\\\date{\\\\today}\\n\\n\\\\begin{document}\\n\\n\\\\maketitle\\n\\n\\\\section{Introduction}\\nThis is a complex example of a LaTeX document showcasing various features.\\n\\n\\\\section{Mathematical Expressions}\\nHere\\'s an example of an integral with limits:\\n\\\\begin{equation}\\n    \\\\int_{0}^{\\\\infty} e^{-x^2} dx = \\\\frac{\\\\sqrt{\\\\pi}}{2}\\n\\\\end{equation}\\nAnd here\\'s a system of equations:\\n\\\\begin{align}\\n    x + y &= 5 \\\\\\\\\\n    2x - y &= 3\\n\\\\end{align}\\n\\n\\\\section{TikZ Diagrams}\\n\\\\begin{figure}[h]\\n    \\\\centering\\n    \\\\begin{tikzpicture}[node distance = 2cm, auto]\\n        % Place nodes\\n        \\\\node [block] (init) {Initialize};\\n        \\\\node [cloud, left of=init] (start) {Start};\\n        \\\\node [cloud, right of=init] (end) {End};\\n        % Draw edges\\n        \\\\path [line] (start) -- (init);\\n        \\\\path [line] (init) -- (end);\\n    \\\\end{tikzpicture}\\n    \\\\caption{Sample TikZ Diagram}\\n    \\\\label{fig:tikz}\\n\\\\end{figure}\\n\\nFigure \\\\ref{fig:tikz} shows a sample TikZ diagram.\\n\\n\\\\section{Custom Environments}\\n\\\\newtheorem{theorem}{Theorem}\\n\\\\begin{theorem}\\nEvery even integer greater than 2 can be expressed as the sum of two primes.\\n\\\\end{theorem}\\n\\n\\\\section{Bibliography}\\nHere are some references:\\n\\\\begin{enumerate}\\n    \\\\item Author A, et al. (Year). Title of the paper. \\\\textit{Journal Name}, Volume(Issue), Page numbers. \\\\url{https://doi.org/xxxxx}\\n    \\\\item Author B, et al. (Year). Title of the paper. \\\\textit{Journal Name}, Volume(Issue), Page numbers. \\\\url{https://doi.org/yyyyy}\\n\\\\end{enumerate}\\n\\n\\\\end{document}\\n\\\\documentclass{article}\\n\\\\usepackage{graphicx}\\n\\\\usepackage{amsmath}\\n\\\\usepackage{listings}\\n\\\\usepackage{enumitem}\\n\\n\\\\title{Advanced LaTeX Example}\\n\\\\author{Your Name}\\n\\\\date{\\\\today}\\n\\n\\\\begin{document}\\n\\n\\\\maketitle\\n\\n\\\\section{Tables}\\n\\\\begin{table}[h]\\n\\\\centering\\n\\\\begin{tabular}{|c|c|c|}\\n\\\\hline\\n\\\\textbf{Name} & \\\\textbf{Age} & \\\\textbf{Gender} \\\\\\\\ \\\\hline\\nAlice & 25 & Female \\\\\\\\ \\\\hline\\nBob & 30 & Male \\\\\\\\ \\\\hline\\nCharlie & 35 & Male \\\\\\\\ \\\\hline\\n\\\\end{tabular}\\n\\\\caption{Sample Table}\\n\\\\label{tab:sample}\\n\\\\end{table}\\n\\nTable \\\\ref{tab:sample} shows a sample table.\\n\\n\\\\section{Matrices}\\n\\\\[\\n\\\\text{Matrix A} = \\n\\\\begin{pmatrix}\\n1 & 2 & 3 \\\\\\\\\\n4 & 5 & 6 \\\\\\\\\\n7 & 8 & 9 \\\\\\\\\\n\\\\end{pmatrix}\\n\\\\]\\n\\n\\\\section{Lists}\\n\\\\subsection{Numbered List}\\n\\\\begin{enumerate}[label=\\\\alph*)]\\n    \\\\item Item 1\\n    \\\\item Item 2\\n    \\\\item Item 3\\n\\\\end{enumerate}\\n\\n\\\\subsection{Nested Bullet Points}\\n\\\\begin{itemize}\\n    \\\\item First level\\n    \\\\begin{itemize}\\n        \\\\item Second level\\n        \\\\begin{itemize}\\n            \\\\item Third level\\n        \\\\end{itemize}\\n    \\\\end{itemize}\\n\\\\end{itemize}\\n\\n\\\\section{Code Listings}\\n\\\\subsection{Python}\\n\\\\lstset{language=Python}\\n\\\\begin{lstlisting}\\ndef fibonacci(n):\\n    if n <= 1:\\n        return n\\n    else:\\n        return fibonacci(n-1) + fibonacci(n-2)\\n\\nprint(fibonacci(10))\\n\\\\end{lstlisting}\\n\\n\\\\subsection{Java}\\n\\\\lstset{language=Java}\\n\\\\begin{lstlisting}\\npublic class HelloWorld {\\n    public static void main(String[] args) {\\n        System.out.println(\"Hello, world!\");\\n    }\\n}\\n\\\\end{lstlisting}\\n\\n\\\\section{Conclusion}\\nThis document demonstrates various advanced features of LaTeX, including tables, matrices, lists, and code listings.\\n\\n\\\\end{document}\\n\\\\documentclass[12pt]{article}\\n\\\\usepackage{lingmacros}\\n\\\\usepackage{tree-dvips}\\n\\\\begin{document}\\n\\n\\\\section*{Notes for My Paper}\\n\\nDon\\'t forget to include examples of topicalization.\\nThey look like this:\\n\\n{\\\\small\\n\\\\enumsentence{Topicalization from sentential subject:\\\\\\\\ \\n\\\\shortex{7}{a John$_i$ [a & kltukl & [el & \\n  {\\\\bf l-}oltoir & er & ngii$_i$ & a Mary]]}\\n{ & {\\\\bf R-}clear & {\\\\sc comp} & \\n  {\\\\bf IR}.{\\\\sc 3s}-love   & P & him & }\\n{John, (it\\'s) clear that Mary loves (him).}}\\n}\\n\\n\\\\subsection*{How to handle topicalization}\\n\\nI\\'ll just assume a tree structure like (\\\\ex{1}).\\n\\n{\\\\small\\n\\\\enumsentence{Structure of A$\\'$ Projections:\\\\\\\\ [2ex]\\n\\\\begin{tabular}[t]{cccc}\\n    & \\\\node{i}{CP}\\\\\\\\ [2ex]\\n    \\\\node{ii}{Spec} &   &\\\\node{iii}{C$\\'$}\\\\\\\\ [2ex]\\n        &\\\\node{iv}{C} & & \\\\node{v}{SAgrP}\\n\\\\end{tabular}\\n\\\\nodeconnect{i}{ii}\\n\\\\nodeconnect{i}{iii}\\n\\\\nodeconnect{iii}{iv}\\n\\\\nodeconnect{iii}{v}\\n}\\n}\\n\\n\\\\subsection*{Mood}\\n\\nMood changes when there is a topic, as well as when\\nthere is WH-movement.  \\\\emph{Irrealis} is the mood when\\nthere is a non-subject topic or WH-phrase in Comp.\\n\\\\emph{Realis} is the mood when there is a subject topic\\nor WH-phrase.\\n\\n\\\\end{document}\\n\\\\documentclass{article}\\n\\\\usepackage{graphicx}\\n\\\\usepackage{amsmath}\\n\\\\usepackage{hyperref}\\n\\\\usepackage{lipsum}\\n\\n\\\\title{Extensive LaTeX Example}\\n\\\\author{Your Name}\\n\\\\date{\\\\today}\\n\\n\\\\begin{document}\\n\\n\\\\maketitle\\n\\n\\\\section{Introduction}\\n\\\\lipsum[1-2]\\n\\n\\\\section{Equations}\\n\\\\subsection{Maxwell\\'s Equations}\\nMaxwell\\'s equations describe how electric and magnetic fields interact with matter. They are given by:\\n\\\\begin{align}\\n    \\\\nabla \\\\cdot \\\\mathbf{E} &= \\\\frac{\\\\rho}{\\\\epsilon_0} \\\\\\\\\\n    \\\\nabla \\\\cdot \\\\mathbf{B} &= 0 \\\\\\\\\\n    \\\\nabla \\\\times \\\\mathbf{E} &= -\\\\frac{\\\\partial \\\\mathbf{B}}{\\\\partial t} \\\\\\\\\\n    \\\\nabla \\\\times \\\\mathbf{B} &= \\\\mu_0\\\\mathbf{J} + \\\\mu_0\\\\epsilon_0\\\\frac{\\\\partial \\\\mathbf{E}}{\\\\partial t}\\n\\\\end{align}\\n\\n\\\\subsection{Schrödinger Equation}\\nThe time-independent Schrödinger equation for a particle with potential energy $V(x)$ is given by:\\n\\\\begin{equation}\\n    -\\\\frac{\\\\hbar^2}{2m} \\\\frac{d^2\\\\psi}{dx^2} + V(x)\\\\psi = E\\\\psi\\n\\\\end{equation}\\n\\n\\\\section{Figures}\\n\\\\begin{figure}[h]\\n    \\\\centering\\n    \\\\includegraphics[width=0.5\\\\textwidth]{example-image-a}\\n    \\\\caption{Example Figure A}\\n    \\\\label{fig:example-a}\\n\\\\end{figure}\\n\\n\\\\lipsum[3]\\n\\n\\\\begin{figure}[h]\\n    \\\\centering\\n    \\\\includegraphics[width=0.5\\\\textwidth]{example-image-b}\\n    \\\\caption{Example Figure B}\\n    \\\\label{fig:example-b}\\n\\\\end{figure}\\n\\n\\\\section{Tables}\\n\\\\begin{table}[h]\\n    \\\\centering\\n    \\\\begin{tabular}{|c|c|}\\n        \\\\hline\\n        \\\\textbf{Name} & \\\\textbf{Age} \\\\\\\\\\n        \\\\hline\\n        Alice & 25 \\\\\\\\\\n        Bob & 30 \\\\\\\\\\n        Charlie & 35 \\\\\\\\\\n        \\\\hline\\n    \\\\end{tabular}\\n    \\\\caption{Sample Table}\\n    \\\\label{tab:sample}\\n\\\\end{table}\\n\\n\\\\lipsum[4-5]\\n\\n\\\\section{References}\\nHere are some references:\\n\\\\begin{enumerate}\\n    \\\\item Author A, et al. (Year). Title of the paper. \\\\textit{Journal Name}, Volume(Issue), Page numbers. \\\\url{https://doi.org/xxxxx}\\n    \\\\item Author B, et al. (Year). Title of the paper. \\\\textit{Journal Name}, Volume(Issue), Page numbers. \\\\url{https://doi.org/yyyyy}\\n\\\\end{enumerate}\\n\\n\\\\end{document}\\n\\\\documentclass{article}\\n\\\\usepackage{graphicx}\\n\\\\usepackage{amsmath}\\n\\\\usepackage{amssymb}\\n\\\\usepackage{lipsum}\\n\\\\usepackage{listings}\\n\\\\usepackage{color}\\n\\\\usepackage{hyperref}\\n\\n\\\\title{Another LaTeX Example}\\n\\\\author{Your Name}\\n\\\\date{\\\\today}\\n\\n\\\\begin{document}\\n\\n\\\\maketitle\\n\\n\\\\section{Custom Environments}\\n\\\\newtheorem{definition}{Definition}\\n\\\\begin{definition}\\nA prime number is a natural number greater than 1 that has no positive divisors other than 1 and itself.\\n\\\\end{definition}\\n\\n\\\\section{Complex Mathematical Expressions}\\nThe Riemann hypothesis states that all non-trivial zeros of the Riemann zeta function have a real part equal to $\\\\frac{1}{2}$.\\n\\n\\\\section{Code Listings}\\n\\\\subsection{Python}\\n\\\\lstset{\\n  language=Python,\\n  basicstyle=\\\\ttfamily,\\n  keywordstyle=\\\\color{blue},\\n  stringstyle=\\\\color{red},\\n  commentstyle=\\\\color{green},\\n  showstringspaces=false,\\n  morekeywords={True, False}\\n}\\n\\\\begin{lstlisting}\\ndef factorial(n):\\n    if n == 0:\\n        return 1\\n    else:\\n        return n * factorial(n-1)\\n\\nprint(factorial(5))\\n\\\\end{lstlisting}\\n\\n\\\\subsection{Java}\\n\\\\lstset{\\n  language=Java,\\n  basicstyle=\\\\ttfamily,\\n  keywordstyle=\\\\color{blue},\\n  stringstyle=\\\\color{red},\\n  commentstyle=\\\\color{green},\\n  showstringspaces=false,\\n  morekeywords={public, class, void, int}\\n}\\n\\\\begin{lstlisting}\\npublic class HelloWorld {\\n    public static void main(String[] args) {\\n        System.out.println(\"Hello, world!\");\\n    }\\n}\\n\\\\end{lstlisting}\\n\\n\\\\section{Bibliography}\\nHere are some references:\\n\\\\begin{enumerate}\\n    \\\\item Author C, et al. (Year). Title of the paper. \\\\textit{Journal Name}, Volume(Issue), Page numbers. \\\\url{https://doi.org/zzzzz}\\n    \\\\item Author D, et al. (Year). Title of the paper. \\\\textit{Journal Name}, Volume(Issue), Page numbers. \\\\url{https://doi.org/wwwww}\\n\\\\end{enumerate}\\n\\n\\\\section{Conclusion}\\nThis document demonstrates various features of LaTeX, including custom environments, code listings, and bibliography management.\\n\\n\\\\end{document}\\n']\n"
          ]
        }
      ],
      "source": [
        "# Verify dataset is loaded correctly\n",
        "print(\"Dataset length:\", len(data))\n",
        "print(\"First 100 characters of dataset:\", data[:100])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z47vcPYyDNqn",
        "outputId": "195482ee-ac2f-437f-afb2-575d98f541aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 100 characters of LaTeX data: \\documentclass{article}\n",
            "\\usepackage{graphicx}\n",
            "\\usepackage{amsmath}\n",
            "\\usepackage{hyperref}\n",
            "\n",
            "\\title{Sam\n",
            "Total length of LaTeX data: 19362\n"
          ]
        }
      ],
      "source": [
        "# If `data` is a list of strings from multiple files, concatenate them into one string\n",
        "# If it's already a single string in a list (as your output suggests), extract it\n",
        "latex_data = ''.join(data)  # Use this if `data` is a list of multiple LaTeX document strings\n",
        "# Or, if `data` is already known to contain exactly one huge string in a list\n",
        "latex_data = data[0]  # Directly extracting the first element\n",
        "\n",
        "# Verify the content and length\n",
        "print(f\"First 100 characters of LaTeX data: {latex_data[:100]}\")\n",
        "print(f\"Total length of LaTeX data: {len(latex_data)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VI0YXXvVA7Mt",
        "outputId": "7140c6f2-1355-4a4b-f9fd-909a781e366e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of dataY: 19322\n",
            "Sample from dataY: [67, 68, 62, 83, 88]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Preprocess the data as before (vectorizing characters, creating sequences)\n",
        "chars = sorted(list(set(latex_data)))\n",
        "char_to_int = {c: i for i, c in enumerate(chars)}\n",
        "int_to_char = {i: c for i, c in enumerate(chars)}\n",
        "seq_length = 40\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, len(latex_data) - seq_length, 1):\n",
        "    seq_in = latex_data[i:i + seq_length]\n",
        "    seq_out = latex_data[i + seq_length]\n",
        "    dataX.append([char_to_int[char] for char in seq_in])\n",
        "    dataY.append(char_to_int[seq_out])\n",
        "\n",
        "\n",
        "print(f\"Length of dataY: {len(dataY)}\")\n",
        "if len(dataY) > 0:\n",
        "    print(f\"Sample from dataY: {dataY[:5]}\")\n",
        "else:\n",
        "    print(\"dataY is empty.\")\n",
        "\n",
        "\n",
        "n_patterns = len(dataX)\n",
        "X = np.reshape(dataX, (n_patterns, seq_length, 1)) / float(len(chars))\n",
        "y = to_categorical(dataY)\n",
        "\n",
        "# Define, compile, and train the RNN model as before\n",
        "# (Refer to the previous model definition and training steps)\n",
        "\n",
        "# After training, use the model to generate LaTeX code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LasIaHKoTLnM"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "frzc46coDcHH",
        "outputId": "3d62c493-1452-431b-8662-16c65b598db4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "61/61 [==============================] - 41s 586ms/step - loss: 3.6518 - val_loss: 3.4761\n",
            "Epoch 2/100\n",
            "61/61 [==============================] - 28s 463ms/step - loss: 3.4888 - val_loss: 3.4648\n",
            "Epoch 3/100\n",
            "61/61 [==============================] - 32s 528ms/step - loss: 3.4803 - val_loss: 3.4680\n",
            "Epoch 4/100\n",
            "61/61 [==============================] - 28s 468ms/step - loss: 3.4732 - val_loss: 3.4642\n",
            "Epoch 5/100\n",
            "61/61 [==============================] - 29s 473ms/step - loss: 3.4573 - val_loss: 3.4719\n",
            "Epoch 6/100\n",
            "61/61 [==============================] - 28s 467ms/step - loss: 3.4481 - val_loss: 3.4274\n",
            "Epoch 7/100\n",
            "61/61 [==============================] - 29s 470ms/step - loss: 3.4115 - val_loss: 3.4017\n",
            "Epoch 8/100\n",
            "61/61 [==============================] - 28s 467ms/step - loss: 3.3726 - val_loss: 3.3489\n",
            "Epoch 9/100\n",
            "61/61 [==============================] - 28s 465ms/step - loss: 3.3355 - val_loss: 3.3039\n",
            "Epoch 10/100\n",
            "61/61 [==============================] - 28s 464ms/step - loss: 3.2922 - val_loss: 3.2815\n",
            "Epoch 11/100\n",
            "61/61 [==============================] - 29s 472ms/step - loss: 3.2617 - val_loss: 3.2525\n",
            "Epoch 12/100\n",
            "61/61 [==============================] - 29s 471ms/step - loss: 3.2375 - val_loss: 3.2267\n",
            "Epoch 13/100\n",
            "61/61 [==============================] - 29s 471ms/step - loss: 3.2057 - val_loss: 3.2228\n",
            "Epoch 14/100\n",
            "61/61 [==============================] - 28s 466ms/step - loss: 3.1892 - val_loss: 3.1878\n",
            "Epoch 15/100\n",
            "61/61 [==============================] - 28s 465ms/step - loss: 3.1566 - val_loss: 3.1771\n",
            "Epoch 16/100\n",
            "61/61 [==============================] - 28s 462ms/step - loss: 3.1381 - val_loss: 3.1556\n",
            "Epoch 17/100\n",
            "61/61 [==============================] - 33s 549ms/step - loss: 3.1061 - val_loss: 3.1309\n",
            "Epoch 18/100\n",
            "61/61 [==============================] - 31s 505ms/step - loss: 3.0785 - val_loss: 3.1012\n",
            "Epoch 19/100\n",
            "61/61 [==============================] - 31s 503ms/step - loss: 3.0484 - val_loss: 3.0848\n",
            "Epoch 20/100\n",
            "61/61 [==============================] - 31s 508ms/step - loss: 3.0192 - val_loss: 3.0634\n",
            "Epoch 21/100\n",
            "61/61 [==============================] - 33s 546ms/step - loss: 2.9846 - val_loss: 3.0315\n",
            "Epoch 22/100\n",
            "61/61 [==============================] - 31s 506ms/step - loss: 2.9618 - val_loss: 3.0081\n",
            "Epoch 23/100\n",
            "61/61 [==============================] - 31s 507ms/step - loss: 2.9265 - val_loss: 2.9881\n",
            "Epoch 24/100\n",
            "61/61 [==============================] - 31s 508ms/step - loss: 2.8903 - val_loss: 2.9527\n",
            "Epoch 25/100\n",
            "61/61 [==============================] - 33s 547ms/step - loss: 2.8629 - val_loss: 2.9202\n",
            "Epoch 26/100\n",
            "61/61 [==============================] - 31s 505ms/step - loss: 2.8203 - val_loss: 2.9010\n",
            "Epoch 27/100\n",
            "61/61 [==============================] - 31s 504ms/step - loss: 2.7832 - val_loss: 2.8687\n",
            "Epoch 28/100\n",
            "61/61 [==============================] - 31s 511ms/step - loss: 2.7417 - val_loss: 2.8424\n",
            "Epoch 29/100\n",
            "61/61 [==============================] - 31s 513ms/step - loss: 2.7047 - val_loss: 2.7943\n",
            "Epoch 30/100\n",
            "61/61 [==============================] - 31s 509ms/step - loss: 2.6643 - val_loss: 2.7706\n",
            "Epoch 31/100\n",
            "61/61 [==============================] - 31s 503ms/step - loss: 2.6199 - val_loss: 2.7430\n",
            "Epoch 32/100\n",
            "61/61 [==============================] - 29s 483ms/step - loss: 2.5828 - val_loss: 2.7084\n",
            "Epoch 33/100\n",
            "61/61 [==============================] - 28s 461ms/step - loss: 2.5419 - val_loss: 2.6775\n",
            "Epoch 34/100\n",
            "61/61 [==============================] - 28s 461ms/step - loss: 2.5013 - val_loss: 2.6603\n",
            "Epoch 35/100\n",
            "61/61 [==============================] - 28s 464ms/step - loss: 2.4499 - val_loss: 2.6314\n",
            "Epoch 36/100\n",
            "61/61 [==============================] - 28s 459ms/step - loss: 2.4134 - val_loss: 2.5976\n",
            "Epoch 37/100\n",
            "61/61 [==============================] - 28s 462ms/step - loss: 2.3737 - val_loss: 2.5813\n",
            "Epoch 38/100\n",
            "61/61 [==============================] - 28s 461ms/step - loss: 2.3302 - val_loss: 2.5553\n",
            "Epoch 39/100\n",
            "61/61 [==============================] - 28s 458ms/step - loss: 2.2836 - val_loss: 2.5286\n",
            "Epoch 40/100\n",
            "61/61 [==============================] - 28s 459ms/step - loss: 2.2520 - val_loss: 2.5030\n",
            "Epoch 41/100\n",
            "61/61 [==============================] - 28s 460ms/step - loss: 2.2195 - val_loss: 2.4911\n",
            "Epoch 42/100\n",
            "61/61 [==============================] - 30s 493ms/step - loss: 2.1830 - val_loss: 2.4606\n",
            "Epoch 43/100\n",
            "61/61 [==============================] - 35s 577ms/step - loss: 2.1488 - val_loss: 2.4495\n",
            "Epoch 44/100\n",
            "61/61 [==============================] - 34s 561ms/step - loss: 2.1101 - val_loss: 2.4297\n",
            "Epoch 45/100\n",
            "61/61 [==============================] - 36s 595ms/step - loss: 2.0573 - val_loss: 2.4064\n",
            "Epoch 46/100\n",
            "61/61 [==============================] - 32s 524ms/step - loss: 2.0220 - val_loss: 2.3890\n",
            "Epoch 47/100\n",
            "61/61 [==============================] - 31s 510ms/step - loss: 1.9900 - val_loss: 2.3758\n",
            "Epoch 48/100\n",
            "61/61 [==============================] - 31s 505ms/step - loss: 1.9633 - val_loss: 2.3769\n",
            "Epoch 49/100\n",
            "61/61 [==============================] - 33s 541ms/step - loss: 1.9234 - val_loss: 2.3630\n",
            "Epoch 50/100\n",
            "61/61 [==============================] - 31s 510ms/step - loss: 1.8948 - val_loss: 2.3537\n",
            "Epoch 51/100\n",
            "61/61 [==============================] - 31s 506ms/step - loss: 1.8629 - val_loss: 2.3224\n",
            "Epoch 52/100\n",
            "61/61 [==============================] - 31s 504ms/step - loss: 1.8263 - val_loss: 2.3268\n",
            "Epoch 53/100\n",
            "61/61 [==============================] - 33s 533ms/step - loss: 1.7932 - val_loss: 2.3171\n",
            "Epoch 54/100\n",
            "61/61 [==============================] - 31s 502ms/step - loss: 1.7879 - val_loss: 2.3122\n",
            "Epoch 55/100\n",
            "61/61 [==============================] - 30s 501ms/step - loss: 1.7440 - val_loss: 2.3032\n",
            "Epoch 56/100\n",
            "61/61 [==============================] - 33s 546ms/step - loss: 1.7064 - val_loss: 2.2899\n",
            "Epoch 57/100\n",
            "61/61 [==============================] - 38s 615ms/step - loss: 1.6883 - val_loss: 2.3054\n",
            "Epoch 58/100\n",
            "61/61 [==============================] - 34s 562ms/step - loss: 1.6612 - val_loss: 2.2833\n",
            "Epoch 59/100\n",
            "61/61 [==============================] - 34s 552ms/step - loss: 1.6422 - val_loss: 2.2975\n",
            "Epoch 60/100\n",
            "61/61 [==============================] - 31s 504ms/step - loss: 1.6119 - val_loss: 2.2662\n",
            "Epoch 61/100\n",
            "61/61 [==============================] - 31s 504ms/step - loss: 1.5922 - val_loss: 2.2815\n",
            "Epoch 62/100\n",
            "61/61 [==============================] - 35s 576ms/step - loss: 1.5484 - val_loss: 2.2686\n",
            "Epoch 63/100\n",
            "61/61 [==============================] - 31s 512ms/step - loss: 1.5331 - val_loss: 2.2818\n",
            "Epoch 64/100\n",
            "61/61 [==============================] - 31s 508ms/step - loss: 1.5153 - val_loss: 2.2627\n",
            "Epoch 65/100\n",
            "61/61 [==============================] - 31s 508ms/step - loss: 1.4789 - val_loss: 2.2577\n",
            "Epoch 66/100\n",
            "61/61 [==============================] - 34s 551ms/step - loss: 1.4665 - val_loss: 2.2844\n",
            "Epoch 67/100\n",
            "61/61 [==============================] - 31s 511ms/step - loss: 1.4358 - val_loss: 2.3080\n",
            "Epoch 68/100\n",
            "61/61 [==============================] - 32s 518ms/step - loss: 1.4195 - val_loss: 2.2657\n",
            "Epoch 69/100\n",
            "61/61 [==============================] - 31s 507ms/step - loss: 1.4078 - val_loss: 2.2923\n",
            "Epoch 70/100\n",
            "61/61 [==============================] - 34s 562ms/step - loss: 1.3749 - val_loss: 2.2821\n",
            "Epoch 71/100\n",
            "61/61 [==============================] - 31s 508ms/step - loss: 1.3586 - val_loss: 2.2835\n",
            "Epoch 72/100\n",
            "61/61 [==============================] - 31s 506ms/step - loss: 1.3330 - val_loss: 2.2818\n",
            "Epoch 73/100\n",
            "61/61 [==============================] - 34s 557ms/step - loss: 1.3178 - val_loss: 2.2841\n",
            "Epoch 74/100\n",
            "61/61 [==============================] - 31s 514ms/step - loss: 1.2997 - val_loss: 2.2866\n",
            "Epoch 75/100\n",
            "61/61 [==============================] - 31s 507ms/step - loss: 1.2770 - val_loss: 2.2821\n",
            "Epoch 76/100\n",
            "61/61 [==============================] - 31s 504ms/step - loss: 1.2586 - val_loss: 2.3035\n",
            "Epoch 77/100\n",
            "61/61 [==============================] - 28s 465ms/step - loss: 1.2336 - val_loss: 2.3066\n",
            "Epoch 78/100\n",
            "61/61 [==============================] - 28s 466ms/step - loss: 1.2218 - val_loss: 2.3146\n",
            "Epoch 79/100\n",
            "61/61 [==============================] - 29s 466ms/step - loss: 1.2109 - val_loss: 2.3043\n",
            "Epoch 80/100\n",
            "61/61 [==============================] - 29s 470ms/step - loss: 1.1811 - val_loss: 2.3086\n",
            "Epoch 81/100\n",
            "61/61 [==============================] - 29s 472ms/step - loss: 1.1728 - val_loss: 2.3004\n",
            "Epoch 82/100\n",
            "61/61 [==============================] - 28s 467ms/step - loss: 1.1579 - val_loss: 2.3365\n",
            "Epoch 83/100\n",
            "61/61 [==============================] - 28s 464ms/step - loss: 1.1432 - val_loss: 2.3347\n",
            "Epoch 84/100\n",
            "61/61 [==============================] - 28s 464ms/step - loss: 1.1284 - val_loss: 2.3343\n",
            "Epoch 85/100\n",
            "61/61 [==============================] - 28s 463ms/step - loss: 1.1122 - val_loss: 2.3509\n",
            "Epoch 86/100\n",
            "61/61 [==============================] - 30s 492ms/step - loss: 1.0856 - val_loss: 2.3429\n",
            "Epoch 87/100\n",
            "61/61 [==============================] - 32s 520ms/step - loss: 1.0656 - val_loss: 2.3399\n",
            "Epoch 88/100\n",
            "61/61 [==============================] - 31s 506ms/step - loss: 1.0770 - val_loss: 2.3845\n",
            "Epoch 89/100\n",
            "61/61 [==============================] - 33s 530ms/step - loss: 1.0476 - val_loss: 2.3844\n",
            "Epoch 90/100\n",
            "61/61 [==============================] - 29s 472ms/step - loss: 1.0421 - val_loss: 2.3993\n",
            "Epoch 91/100\n",
            "61/61 [==============================] - 28s 468ms/step - loss: 1.0290 - val_loss: 2.4180\n",
            "Epoch 92/100\n",
            "61/61 [==============================] - 28s 466ms/step - loss: 1.0079 - val_loss: 2.4023\n",
            "Epoch 93/100\n",
            "61/61 [==============================] - 30s 499ms/step - loss: 0.9834 - val_loss: 2.3965\n",
            "Epoch 94/100\n",
            "61/61 [==============================] - 32s 531ms/step - loss: 0.9929 - val_loss: 2.3866\n",
            "Epoch 95/100\n",
            "61/61 [==============================] - 34s 564ms/step - loss: 0.9701 - val_loss: 2.4211\n",
            "Epoch 96/100\n",
            "61/61 [==============================] - 34s 564ms/step - loss: 0.9602 - val_loss: 2.4328\n",
            "Epoch 97/100\n",
            "61/61 [==============================] - 29s 475ms/step - loss: 0.9381 - val_loss: 2.4061\n",
            "Epoch 98/100\n",
            "61/61 [==============================] - 28s 466ms/step - loss: 0.9364 - val_loss: 2.4486\n",
            "Epoch 99/100\n",
            "61/61 [==============================] - 29s 470ms/step - loss: 0.9238 - val_loss: 2.4479\n",
            "Epoch 100/100\n",
            "61/61 [==============================] - 29s 476ms/step - loss: 0.9250 - val_loss: 2.4416\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d4be6f12380>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dropout, Dense\n",
        "\n",
        "# Define the RNN model\n",
        "model = Sequential([\n",
        "    LSTM(128, input_shape=(X.shape[1], X.shape[2]), return_sequences=True),\n",
        "    Dropout(0.2),\n",
        "    LSTM(128),\n",
        "    Dropout(0.2),\n",
        "    # Final output layer\n",
        "    Dense(y.shape[1], activation='softmax'),  # Keep this layer unchanged\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# Callbacks\n",
        "#from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "#early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min', restore_best_weights=True)\n",
        "#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, verbose=1, mode='min')\n",
        "\n",
        "# Train the model with early stopping and learning rate reduction\n",
        "#model.fit(X, y, epochs=100, batch_size=256, callbacks=[early_stopping, reduce_lr], validation_split=0.2)\n",
        "model.fit(X, y, epochs=100, batch_size=256,  validation_split=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0vGECzqmMb8H",
        "outputId": "9e3027d6-1fd4-4348-9cfa-1b128fc8c1b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 803ms/step\n"
          ]
        }
      ],
      "source": [
        "def adjust_input_sequence(seq, target_length=40):\n",
        "    # Pad the sequence if it's shorter than the target length\n",
        "    padding = [0] * (target_length - len(seq))\n",
        "    padded_seq = seq + padding\n",
        "    return padded_seq\n",
        "\n",
        "# Example usage\n",
        "input_seq = [char_to_int[char] for char in \"your_input_sequence_here\"]\n",
        "adjusted_seq = adjust_input_sequence(input_seq)\n",
        "adjusted_seq = np.reshape(adjusted_seq, (1, 40, 1))  # Reshaping to match the input shape expected by the model\n",
        "\n",
        "# Now using adjusted_seq for prediction\n",
        "prediction = model.predict(adjusted_seq)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Lt14IhOmM2s9",
        "outputId": "89da4590-eb0e-40ae-bf41-6e4b611900a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \\section{Sigure}\n",
            "\\uebtections:\n",
            "\\regr \\refeent daathres.oure}atio \n",
            "\\eec donueens siuese to inpr piis aiatr.\n",
            "\n",
            "\\eeccion{Lusereocoio}\n",
            "Thid{fofo}\n",
            "\\erg{ettmaat}\n",
            "\n",
            "\\section{cent}\n",
            "\\dogsettillss{articele}\n",
            "\\usepcckage{amssath}\n",
            "\\usepackage{lissing}\n",
            "\\usepackage{tippri}\n",
            "\\usepackage{lissiigs}\n",
            "\\usepackage{tiopat}\n",
            "\\usepackage{tiopr\n",
            "\\usepackage{aissrcn}\n",
            "\\usepackage{tiksiin}\n",
            "\\usepackage{tiopri}\n",
            "\\usepackage{lissiigs\n"
          ]
        }
      ],
      "source": [
        "def generate_latex(model, char_to_int, int_to_char, seed_text, sequence_length, num_chars_to_generate=400):\n",
        "    \"\"\"\n",
        "    Generate LaTeX code using a trained RNN model.\n",
        "\n",
        "    Parameters:\n",
        "    - model: The trained RNN model.\n",
        "    - char_to_int: Dictionary mapping characters to integers.\n",
        "    - int_to_char: Dictionary mapping integers back to characters.\n",
        "    - seed_text: A seed sequence to start generating from.\n",
        "    - sequence_length: The length of sequences the model was trained with.\n",
        "    - num_chars_to_generate: Number of characters to generate.\n",
        "\n",
        "    Returns:\n",
        "    - generated_text: The generated LaTeX code as a string.\n",
        "    \"\"\"\n",
        "    generated_text = \"\"\n",
        "    # Adjust seed text to match the expected sequence length\n",
        "    if len(seed_text) > sequence_length:\n",
        "        seed_text = seed_text[-sequence_length:]\n",
        "    elif len(seed_text) < sequence_length:\n",
        "        # Pad seed_text if it's shorter than required\n",
        "        seed_text = seed_text.rjust(sequence_length)\n",
        "\n",
        "    for _ in range(num_chars_to_generate):\n",
        "        # Convert the seed text to a sequence of integers\n",
        "        input_seq = [char_to_int.get(char, 0) for char in seed_text]  # Use .get() to handle unseen chars\n",
        "        input_seq = np.reshape(input_seq, (1, sequence_length, 1))\n",
        "        input_seq = input_seq / float(len(char_to_int))\n",
        "\n",
        "        # Predict the next character\n",
        "        prediction = model.predict(input_seq, verbose=0)\n",
        "        index = np.argmax(prediction)\n",
        "        next_char = int_to_char[index]\n",
        "\n",
        "        generated_text += next_char\n",
        "        seed_text = seed_text[1:] + next_char  # Update seed text\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Adjust the sequence_length accordingly\n",
        "sequence_length = 40  # This should match the training sequence length\n",
        "seed_text = \"\\\\begin{document}\\n\"  # Your initial seed text\n",
        "# Make sure to pass the sequence_length to the function\n",
        "generated_latex = generate_latex(model, char_to_int, int_to_char, seed_text, sequence_length, num_chars_to_generate=400)\n",
        "print(generated_latex)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NG2DNwTINwPh",
        "outputId": "cb06c024-bb98-4d36-ebae-e71548238574"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                                                                                                                                                                                                                                                                                                                                                                                \n"
          ]
        }
      ],
      "source": [
        "print(generated_latex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipRm-mvDOIAG",
        "outputId": "3dd433c6-187d-4437-e054-b003ece91f69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 36ms/step\n",
            "Model prediction successful.\n"
          ]
        }
      ],
      "source": [
        "# Create a dummy input sequence that matches the model's expected input shape\n",
        "dummy_input = np.zeros((1, 40, 1))  # Assuming 40 is the sequence length your model was trained on\n",
        "\n",
        "# Try making a prediction with the dummy input\n",
        "try:\n",
        "    dummy_prediction = model.predict(dummy_input)\n",
        "    print(\"Model prediction successful.\")\n",
        "except Exception as e:\n",
        "    print(f\"Model prediction failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_ZOQRh5UOf-t"
      },
      "outputs": [],
      "source": [
        "def generate_latex(model, char_to_int, int_to_char, seed_text, sequence_length, num_chars_to_generate=400):\n",
        "    \"\"\"\n",
        "    Generate LaTeX code using a trained RNN model.\n",
        "\n",
        "    Parameters:\n",
        "    - model: The trained RNN model.\n",
        "    - char_to_int: Dictionary mapping characters to integers.\n",
        "    - int_to_char: Dictionary mapping integers back to characters.\n",
        "    - seed_text: A seed sequence to start generating from.\n",
        "    - sequence_length: The length of sequences the model was trained with.\n",
        "    - num_chars_to_generate: Number of characters to generate.\n",
        "\n",
        "    Returns:\n",
        "    - generated_text: The generated LaTeX code as a string.\n",
        "    \"\"\"\n",
        "    generated_text = \"\"\n",
        "\n",
        "    # Adjust the seed text to the sequence length expected by the model\n",
        "    seed_text = seed_text[-sequence_length:]  # Use the last 'sequence_length' characters\n",
        "    if len(seed_text) < sequence_length:\n",
        "        # Pad the seed_text if shorter than expected\n",
        "        padding = ' ' * (sequence_length - len(seed_text))  # Assuming space can be used for padding\n",
        "        seed_text = padding + seed_text\n",
        "\n",
        "    for _ in range(num_chars_to_generate):\n",
        "        # Convert the seed text to a sequence of integers\n",
        "        input_seq = [char_to_int.get(char, 0) for char in seed_text]  # Use .get() to handle unseen chars\n",
        "        input_seq = np.reshape(input_seq, (1, sequence_length, 1))\n",
        "        input_seq = input_seq / float(len(char_to_int))\n",
        "\n",
        "        # Predict the next character\n",
        "        prediction = model.predict(input_seq, verbose=0)\n",
        "        index = np.argmax(prediction)\n",
        "        next_char = int_to_char[index]\n",
        "\n",
        "        generated_text += next_char\n",
        "        seed_text = seed_text[1:] + next_char  # Update seed text with the next character\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Example usage\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1WYul4RaOwAA",
        "outputId": "619f89c4-1348-488f-982b-6cab9a49f55a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \\section{Sigure}\n",
            "\\uebtections:\n",
            "\\regr \\refeent daathres.oure}atio \n",
            "\\eec donueens siuese to inpr piis aiatr.\n",
            "\n",
            "\\eeccion{Lusereocoio}\n",
            "Thid{fofo}\n",
            "\\erg{ettmaat}\n",
            "\n",
            "\\section{cent}\n",
            "\\dogsettillss{articele}\n",
            "\\usepcckage{amssath}\n",
            "\\usepackage{lissing}\n",
            "\\usepackage{tippri}\n",
            "\\usepackage{lissiigs}\n",
            "\\usepackage{tiopat}\n",
            "\\usepackage{tiopr\n",
            "\\usepackage{aissrcn}\n",
            "\\usepackage{tiksiin}\n",
            "\\usepackage{tiopri}\n",
            "\\usepackage{lissiigs\n"
          ]
        }
      ],
      "source": [
        "seed_text = \"\\\\begin{document}\\n\"  # Adjust your seed text as needed\n",
        "sequence_length = 40  # This should match the sequence length your model expects\n",
        "generated_latex = generate_latex(model, char_to_int, int_to_char, seed_text, sequence_length, num_chars_to_generate=400)\n",
        "print(generated_latex)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}